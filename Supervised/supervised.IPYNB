{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6c81902",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "60d5286b",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '66acd42635424_r3_data.xlsx'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "63af83c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#since the file is password protected\n",
    "import msoffcrypto\n",
    "import io\n",
    "passwd = 'MERCHANT'\n",
    "\n",
    "decrypted_workbook = io.BytesIO()\n",
    "with open(file_path, 'rb') as file:\n",
    "    office_file = msoffcrypto.OfficeFile(file)\n",
    "    office_file.load_key(password=passwd)\n",
    "    office_file.decrypt(decrypted_workbook)\n",
    "\n",
    "df = pd.read_excel(decrypted_workbook, sheet_name='Data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "52d7fd0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unique_identifier</th>\n",
       "      <th>appl_month</th>\n",
       "      <th>prod_name</th>\n",
       "      <th>acq_channel</th>\n",
       "      <th>state_code</th>\n",
       "      <th>bureau_src</th>\n",
       "      <th>risk_score_1</th>\n",
       "      <th>bureau_score</th>\n",
       "      <th>limit</th>\n",
       "      <th>income</th>\n",
       "      <th>...</th>\n",
       "      <th>merchant2_cat</th>\n",
       "      <th>merchant3</th>\n",
       "      <th>merchant3_amt</th>\n",
       "      <th>merchant3_ten</th>\n",
       "      <th>merchant3_cat</th>\n",
       "      <th>risk_score_8</th>\n",
       "      <th>risk_score_9</th>\n",
       "      <th>risk_score_10</th>\n",
       "      <th>risk_score_11</th>\n",
       "      <th>default_ind</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>116356</td>\n",
       "      <td>Feb</td>\n",
       "      <td>Product 7</td>\n",
       "      <td>Channel 4</td>\n",
       "      <td>CA</td>\n",
       "      <td>No Bureau Hit</td>\n",
       "      <td>0.0</td>\n",
       "      <td>800</td>\n",
       "      <td>5000</td>\n",
       "      <td>45000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.2787</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>110872</td>\n",
       "      <td>Feb</td>\n",
       "      <td>Product 6</td>\n",
       "      <td>Channel 3</td>\n",
       "      <td>GA</td>\n",
       "      <td>Bureau 1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>775</td>\n",
       "      <td>13000</td>\n",
       "      <td>100000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.999</td>\n",
       "      <td>0.0092</td>\n",
       "      <td>0.070</td>\n",
       "      <td>0.2275</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>158686</td>\n",
       "      <td>Jun</td>\n",
       "      <td>Product 6</td>\n",
       "      <td>Channel 1</td>\n",
       "      <td>CA</td>\n",
       "      <td>Bureau 1</td>\n",
       "      <td>5.6</td>\n",
       "      <td>900</td>\n",
       "      <td>2000</td>\n",
       "      <td>100000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.0015</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.2787</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>148772</td>\n",
       "      <td>May</td>\n",
       "      <td>Product 5</td>\n",
       "      <td>Channel 3</td>\n",
       "      <td>DC</td>\n",
       "      <td>No Bureau Hit</td>\n",
       "      <td>0.0</td>\n",
       "      <td>800</td>\n",
       "      <td>5000</td>\n",
       "      <td>170000</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.252</td>\n",
       "      <td>0.0253</td>\n",
       "      <td>0.131</td>\n",
       "      <td>0.1175</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>158473</td>\n",
       "      <td>Jun</td>\n",
       "      <td>Product 7</td>\n",
       "      <td>Channel 2</td>\n",
       "      <td>VA</td>\n",
       "      <td>Bureau 1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>775</td>\n",
       "      <td>10000</td>\n",
       "      <td>40000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.2035</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows √ó 61 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   unique_identifier appl_month  prod_name acq_channel state_code  \\\n",
       "0             116356        Feb  Product 7   Channel 4         CA   \n",
       "1             110872        Feb  Product 6   Channel 3         GA   \n",
       "2             158686        Jun  Product 6   Channel 1         CA   \n",
       "3             148772        May  Product 5   Channel 3         DC   \n",
       "4             158473        Jun  Product 7   Channel 2         VA   \n",
       "\n",
       "      bureau_src  risk_score_1  bureau_score  limit  income  ...  \\\n",
       "0  No Bureau Hit           0.0           800   5000   45000  ...   \n",
       "1       Bureau 1           0.1           775  13000  100000  ...   \n",
       "2       Bureau 1           5.6           900   2000  100000  ...   \n",
       "3  No Bureau Hit           0.0           800   5000  170000  ...   \n",
       "4       Bureau 1           0.1           775  10000   40000  ...   \n",
       "\n",
       "   merchant2_cat  merchant3  merchant3_amt  merchant3_ten  merchant3_cat  \\\n",
       "0              0          0            0.0              0              0   \n",
       "1              0          0            0.0              0              0   \n",
       "2              0          0            0.0              0              0   \n",
       "3              1          0            0.0              0              0   \n",
       "4              0          0            0.0              0              0   \n",
       "\n",
       "   risk_score_8  risk_score_9 risk_score_10  risk_score_11  default_ind  \n",
       "0         0.000        0.0000         0.012         0.2787            0  \n",
       "1         0.999        0.0092         0.070         0.2275            0  \n",
       "2         0.002        0.0015         0.035         0.2787            1  \n",
       "3         0.252        0.0253         0.131         0.1175            0  \n",
       "4         0.000        0.0000         0.000         0.2035            0  \n",
       "\n",
       "[5 rows x 61 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7be53c84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(62484, 61)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7bf0a8e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  feature        iv\n",
      "0            risk_score_2  1.480566\n",
      "1            addr_changes  1.421535\n",
      "2                   dl_id  1.161068\n",
      "3          no_phn_to_addr  1.082154\n",
      "4            risk_score_3  0.963746\n",
      "5            risk_score_7  0.942674\n",
      "6            risk_score_1  0.762850\n",
      "7          no_uid_to_addr  0.717424\n",
      "8          no_nid_to_addr  0.712828\n",
      "9            risk_score_5  0.665466\n",
      "10         no_addr_to_nid  0.534604\n",
      "11          merchant1_amt  0.531288\n",
      "12            decline_txn  0.517154\n",
      "13           basic_no_trd  0.505185\n",
      "14       uid_addr_matches  0.409201\n",
      "15  basic_old_open_trd_ms  0.401405\n",
      "16                addr_ds  0.369292\n",
      "17           risk_score_6  0.361616\n",
      "18            no_open_trd  0.344031\n",
      "19          merchant2_amt  0.340388\n",
      "20           risk_score_4  0.319124\n",
      "21          merchant1_ten  0.298438\n",
      "22                 no_trd  0.297536\n",
      "23              merchant1  0.277623\n",
      "24             no_trd_del  0.266868\n",
      "25        uid_nid_matches  0.238311\n",
      "26          income_incons  0.217015\n",
      "27               payments  0.214154\n",
      "28      basic_max_trd_tnr  0.213309\n",
      "29          merchant3_amt  0.183882\n",
      "30        old_open_trd_ms  0.182645\n",
      "31           bureau_score  0.180589\n",
      "32                  spend  0.172651\n",
      "33                 income  0.166852\n",
      "34          risk_score_11  0.164670\n",
      "35              merchant2  0.146721\n",
      "36                  limit  0.132394\n",
      "37          merchant1_cat  0.115485\n",
      "38               debt_cap  0.109237\n",
      "39              merchant3  0.105720\n",
      "40            attempt_txn  0.060158\n",
      "41          merchant2_ten  0.049894\n",
      "42             home_value  0.047899\n",
      "43          merchant3_ten  0.039972\n",
      "44           risk_score_9  0.028230\n",
      "45                 no_inq  0.026076\n",
      "46           risk_score_8  0.018750\n",
      "47          risk_score_10  0.013547\n",
      "48          merchant2_cat  0.012277\n",
      "49        bureau_mismatch  0.010136\n",
      "50          merchant3_cat  0.002867\n",
      "51        return_payments  0.000000\n"
     ]
    }
   ],
   "source": [
    "# compute IV (Information Value) for all numeric features vs default_ind\n",
    "# assumes `df`, `np`, `pd` are already in the notebook namespace\n",
    "\n",
    "def _calc_iv_for_series(x, y, bins=10, eps=1e-6):\n",
    "    \"\"\"Return IV and a breakdown DataFrame for a single numeric series x vs binary target y.\"\"\"\n",
    "    # handle missing\n",
    "    ser = x.copy()\n",
    "    # choose binning strategy\n",
    "    if ser.nunique(dropna=True) > bins:\n",
    "        try:\n",
    "            binned = pd.qcut(ser, q=bins, duplicates='drop')\n",
    "        except Exception:\n",
    "            binned = pd.cut(ser, bins=bins)\n",
    "    else:\n",
    "        # if few unique values, use them as categories\n",
    "        binned = ser.astype(object)\n",
    "\n",
    "    binned = binned.astype(str).fillna('Missing')\n",
    "    grp = pd.concat([binned.rename('bin'), y.rename('target')], axis=1).groupby('bin')['target'].agg(['count', 'sum'])\n",
    "    grp = grp.rename(columns={'sum': 'events', 'count': 'total'})\n",
    "    grp['non_events'] = grp['total'] - grp['events']\n",
    "\n",
    "    total_events = grp['events'].sum()\n",
    "    total_non_events = grp['non_events'].sum()\n",
    "    if total_events == 0 or total_non_events == 0:\n",
    "        return np.nan, grp  # cannot compute IV\n",
    "\n",
    "    # rates\n",
    "    grp['event_rate'] = grp['events'] / total_events\n",
    "    grp['non_event_rate'] = grp['non_events'] / total_non_events\n",
    "\n",
    "    # avoid zeros\n",
    "    grp['event_rate'] = grp['event_rate'].replace(0, eps)\n",
    "    grp['non_event_rate'] = grp['non_event_rate'].replace(0, eps)\n",
    "\n",
    "    grp['woe'] = np.log(grp['event_rate'] / grp['non_event_rate'])\n",
    "    grp['iv_bin'] = (grp['event_rate'] - grp['non_event_rate']) * grp['woe']\n",
    "    iv = grp['iv_bin'].sum()\n",
    "    return iv, grp.sort_values(by='event_rate', ascending=False)\n",
    "\n",
    "# select numeric features, exclude target and obvious id\n",
    "target_col = 'default_ind'\n",
    "numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "numeric_cols = [c for c in numeric_cols if c != target_col and c != 'unique_identifier']\n",
    "\n",
    "iv_results = []\n",
    "iv_details = {}\n",
    "\n",
    "for col in numeric_cols:\n",
    "    if df[col].nunique(dropna=True) <= 1:\n",
    "        continue\n",
    "    iv, detail = _calc_iv_for_series(df[col], df[target_col], bins=10)\n",
    "    iv_results.append((col, iv))\n",
    "    iv_details[col] = detail\n",
    "\n",
    "iv_df = pd.DataFrame(iv_results, columns=['feature', 'iv']).sort_values('iv', ascending=False).reset_index(drop=True)\n",
    "\n",
    "# show IV table\n",
    "print(iv_df)\n",
    "\n",
    "# optional: access bin-level WOE/IV for a specific feature, e.g.:\n",
    "# iv_details['income']   # uncomment to inspect per-bin breakdown for 'income'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5b8f290d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "strong (21): ['risk_score_2', 'addr_changes', 'dl_id', 'no_phn_to_addr', 'risk_score_3', 'risk_score_7', 'risk_score_1', 'no_uid_to_addr', 'no_nid_to_addr', 'risk_score_5']\n",
      "medium (19): ['merchant1_ten', 'no_trd', 'merchant1', 'no_trd_del', 'uid_nid_matches', 'income_incons', 'payments', 'basic_max_trd_tnr', 'merchant3_amt', 'old_open_trd_ms']\n",
      "weak (12): ['attempt_txn', 'merchant2_ten', 'home_value', 'merchant3_ten', 'risk_score_9', 'no_inq', 'risk_score_8', 'risk_score_10', 'merchant2_cat', 'bureau_mismatch']\n"
     ]
    }
   ],
   "source": [
    "# categorize features by IV into strong / medium / weak\n",
    "# thresholds (tunable): weak < weak_thr, medium in [weak_thr, strong_thr), strong >= strong_thr\n",
    "weak_thr = 0.10\n",
    "strong_thr = 0.30\n",
    "\n",
    "# use existing iv_df if available, otherwise fallback to iv_results\n",
    "if 'iv_df' in globals():\n",
    "    iv_source = iv_df.copy()\n",
    "else:\n",
    "    iv_source = pd.DataFrame(iv_results, columns=['feature', 'iv'])\n",
    "\n",
    "iv_source = iv_source.dropna(subset=['iv'])\n",
    "\n",
    "strong_features = iv_source[iv_source['iv'] >= strong_thr]['feature'].tolist()\n",
    "medium_features = iv_source[(iv_source['iv'] >= weak_thr) & (iv_source['iv'] < strong_thr)]['feature'].tolist()\n",
    "weak_features = iv_source[iv_source['iv'] < weak_thr]['feature'].tolist()\n",
    "\n",
    "# expose lists in notebook namespace and print a short summary\n",
    "print(f\"strong ({len(strong_features)}): {strong_features[:10]}\")\n",
    "print(f\"medium ({len(medium_features)}): {medium_features[:10]}\")\n",
    "print(f\"weak ({len(weak_features)}): {weak_features[:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dbc23b3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "default_ind\n",
       "0    62121\n",
       "1      363\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['default_ind'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cb3b7d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import xlsxwriter\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Fallbacks if not defined earlier\n",
    "if 'target_col' not in globals():\n",
    "    target_col = 'default_ind'\n",
    "\n",
    "if 'numeric_cols' not in globals():\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    numeric_cols = [c for c in numeric_cols if c != target_col and c != 'unique_identifier']\n",
    "\n",
    "# IV + binning helper if not already defined\n",
    "if '_calc_iv_for_series' not in globals():\n",
    "    def _calc_iv_for_series(x, y, bins=10, eps=1e-6):\n",
    "        ser = x.copy()\n",
    "\n",
    "        if ser.nunique(dropna=True) > bins:\n",
    "            try:\n",
    "                binned = pd.qcut(ser, q=bins, duplicates='drop')\n",
    "            except Exception:\n",
    "                binned = pd.cut(ser, bins=bins)\n",
    "        else:\n",
    "            binned = ser.astype(object)\n",
    "\n",
    "        binned = binned.astype(str).fillna('Missing')\n",
    "\n",
    "        grp = pd.concat([binned.rename('bin'), y.rename('target')], axis=1) \\\n",
    "                .groupby('bin')['target'].agg(['count', 'sum'])\n",
    "\n",
    "        grp = grp.rename(columns={'sum': 'events', 'count': 'total'})\n",
    "        grp['non_events'] = grp['total'] - grp['events']\n",
    "\n",
    "        total_events = grp['events'].sum()\n",
    "        total_non_events = grp['non_events'].sum()\n",
    "\n",
    "        if total_events == 0 or total_non_events == 0:\n",
    "            return np.nan, grp\n",
    "\n",
    "        grp['event_rate'] = grp['events'] / total_events\n",
    "        grp['non_event_rate'] = grp['non_events'] / total_non_events\n",
    "\n",
    "        grp['event_rate'] = grp['event_rate'].replace(0, eps)\n",
    "        grp['non_event_rate'] = grp['non_event_rate'].replace(0, eps)\n",
    "\n",
    "        grp['woe'] = np.log(grp['event_rate'] / grp['non_event_rate'])\n",
    "        grp['iv_bin'] = (grp['event_rate'] - grp['non_event_rate']) * grp['woe']\n",
    "\n",
    "        iv = grp['iv_bin'].sum()\n",
    "        return iv, grp.sort_values(by='event_rate', ascending=False)\n",
    "\n",
    "\n",
    "# Output Excel file\n",
    "out_xlsx = 'numeric_bivariate_10bin.xlsx'\n",
    "\n",
    "with pd.ExcelWriter(out_xlsx, engine='xlsxwriter') as writer:\n",
    "    workbook = writer.book\n",
    "\n",
    "    for col in numeric_cols:\n",
    "\n",
    "        if df[col].nunique(dropna=True) <= 1:\n",
    "            continue\n",
    "\n",
    "        iv, detail = _calc_iv_for_series(df[col], df[target_col], bins=10)\n",
    "\n",
    "        detail_df = detail.reset_index().rename(columns={'index': 'bin'})\n",
    "\n",
    "        sheet_name = str(col)[:31]\n",
    "        detail_df.to_excel(writer, sheet_name=sheet_name, startrow=0, startcol=0, index=False)\n",
    "\n",
    "        # --------------------------- PLOT ---------------------------\n",
    "        fig, ax = plt.subplots(figsize=(10, 4))\n",
    "\n",
    "        x = np.arange(len(detail_df))\n",
    "        width = 0.35\n",
    "\n",
    "        ax.bar(x - width/2, detail_df['event_rate'], width=width, label='event_rate', color='#d62728')\n",
    "        ax.bar(x + width/2, detail_df['non_event_rate'], width=width, label='non_event_rate', color='#1f77b4')\n",
    "\n",
    "        ax.set_ylabel('Rate (normalized)')\n",
    "        ax.set_xticks(x)\n",
    "        ax.set_xticklabels(detail_df['bin'], rotation=45, ha='right', fontsize=8)\n",
    "\n",
    "        # ------------ UPDATED LINE CHART ‚Üí EVENTS PER BIN ------------\n",
    "        ax2 = ax.twinx()\n",
    "        ax2.plot(x, detail_df['events'], color='black', marker='o', label='events')\n",
    "        ax2.set_ylabel('Number of Events')\n",
    "\n",
    "        # Legend merge\n",
    "        handles1, labels1 = ax.get_legend_handles_labels()\n",
    "        handles2, labels2 = ax2.get_legend_handles_labels()\n",
    "        ax.legend(handles1 + handles2, labels1 + labels2, loc='upper right', fontsize=8)\n",
    "\n",
    "        fig.tight_layout()\n",
    "\n",
    "        # Save plot into Excel\n",
    "        imgdata = io.BytesIO()\n",
    "        plt.savefig(imgdata, format='png', dpi=150)\n",
    "        plt.close(fig)\n",
    "        imgdata.seek(0)\n",
    "\n",
    "        worksheet = writer.sheets[sheet_name]\n",
    "        img_row = len(detail_df) + 3\n",
    "\n",
    "        worksheet.insert_image(img_row, 0, f'{col}.png',\n",
    "                               {'image_data': imgdata, 'x_scale': 1.0, 'y_scale': 1.0})\n",
    "\n",
    "# END\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e594b424",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import xlsxwriter\n",
    "\n",
    "# Cell to save bivariate plots + 10-bin breakdowns for every numeric feature into an Excel file.\n",
    "# Assumes `df`, `numeric_cols`, `target_col` and (optionally) `_calc_iv_for_series` are already defined in the notebook.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# fallbacks if previous cells didn't create these names\n",
    "if 'target_col' not in globals():\n",
    "    target_col = 'default_ind'\n",
    "if 'numeric_cols' not in globals():\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    numeric_cols = [c for c in numeric_cols if c != target_col and c != 'unique_identifier']\n",
    "\n",
    "# reuse existing IV/binning helper if available, otherwise provide a minimal compatible one\n",
    "if '_calc_iv_for_series' not in globals():\n",
    "    def _calc_iv_for_series(x, y, bins=10, eps=1e-6):\n",
    "        ser = x.copy()\n",
    "        if ser.nunique(dropna=True) > bins:\n",
    "            try:\n",
    "                binned = pd.qcut(ser, q=bins, duplicates='drop')\n",
    "            except Exception:\n",
    "                binned = pd.cut(ser, bins=bins)\n",
    "        else:\n",
    "            binned = ser.astype(object)\n",
    "        binned = binned.astype(str).fillna('Missing')\n",
    "        grp = pd.concat([binned.rename('bin'), y.rename('target')], axis=1).groupby('bin')['target'].agg(['count', 'sum'])\n",
    "        grp = grp.rename(columns={'sum': 'events', 'count': 'total'})\n",
    "        grp['non_events'] = grp['total'] - grp['events']\n",
    "        total_events = grp['events'].sum()\n",
    "        total_non_events = grp['non_events'].sum()\n",
    "        if total_events == 0 or total_non_events == 0:\n",
    "            return np.nan, grp\n",
    "        grp['event_rate'] = grp['events'] / (grp['events']+group['non_events'])\n",
    "        grp['non_event_rate'] = grp['non_events'] / (grp['events']+group['non_events'])\n",
    "        grp['event_rate'] = grp['event_rate'].replace(0, eps)\n",
    "        grp['non_event_rate'] = grp['non_event_rate'].replace(0, eps)\n",
    "        grp['woe'] = np.log(grp['event_rate'] / grp['non_event_rate'])\n",
    "        grp['iv_bin'] = (grp['event_rate'] - grp['non_event_rate']) * grp['woe']\n",
    "        iv = grp['iv_bin'].sum()\n",
    "        return iv, grp.sort_values(by='event_rate', ascending=False)\n",
    "\n",
    "out_xlsx = 'numeric_bivariate_10bin.xlsx'\n",
    "with pd.ExcelWriter(out_xlsx, engine='xlsxwriter') as writer:\n",
    "    workbook = writer.book\n",
    "    for col in numeric_cols:\n",
    "        # skip trivial columns\n",
    "        if df[col].nunique(dropna=True) <= 1:\n",
    "            continue\n",
    "\n",
    "        iv, detail = _calc_iv_for_series(df[col], df[target_col], bins=10)\n",
    "        # prepare dataframe to write (make bin a column)\n",
    "        detail_df = detail.reset_index().rename(columns={'index': 'bin'})\n",
    "\n",
    "        # create a safe sheet name (max 31 chars)\n",
    "        sheet_name = str(col)[:31]\n",
    "        detail_df.to_excel(writer, sheet_name=sheet_name, startrow=0, startcol=0, index=False)\n",
    "\n",
    "        # build bivariate plot: event_rate & non_event_rate bars, total counts line\n",
    "        fig, ax = plt.subplots(figsize=(10, 4))\n",
    "        x = np.arange(len(detail_df))\n",
    "        width = 0.35\n",
    "        ax.bar(x - width/2, detail_df['event_rate'], width=width, label='event_rate', color='#d62728')\n",
    "        ax.bar(x + width/2, detail_df['non_event_rate'], width=width, label='non_event_rate', color='#1f77b4')\n",
    "        ax.set_ylabel('Rate (normalized)')\n",
    "        ax.set_xticks(x)\n",
    "        ax.set_xticklabels(detail_df['bin'], rotation=45, ha='right', fontsize=8)\n",
    "\n",
    "        ax2 = ax.twinx()\n",
    "        ax2.plot(x, detail_df['total'], color='black', marker='o', label='total')\n",
    "        ax2.set_ylabel('Count')\n",
    "\n",
    "        # legends\n",
    "        handles1, labels1 = ax.get_legend_handles_labels()\n",
    "        handles2, labels2 = ax2.get_legend_handles_labels()\n",
    "        ax.legend(handles1 + handles2, labels1 + labels2, loc='upper right', fontsize=8)\n",
    "        fig.tight_layout()\n",
    "\n",
    "        # save figure to bytes and insert into Excel\n",
    "        imgdata = io.BytesIO()\n",
    "        plt.savefig(imgdata, format='png', dpi=150)\n",
    "        plt.close(fig)\n",
    "        imgdata.seek(0)\n",
    "\n",
    "        worksheet = writer.sheets[sheet_name]\n",
    "        # place image after the table; table height = len(detail_df) + header row\n",
    "        img_row = len(detail_df) + 3\n",
    "        worksheet.insert_image(img_row, 0, f'{col}.png', {'image_data': imgdata, 'x_scale': 1.0, 'y_scale': 1.0})\n",
    "\n",
    "# done - the Excel file numeric_bivariate_10bin.xlsx contains one sheet per numeric feature with the 10-bin breakdown and the bivariate plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eb90824b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nityam\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\imblearn\\over_sampling\\_smote\\base.py:370: FutureWarning: The parameter `n_jobs` has been deprecated in 0.10 and will be removed in 0.12. You can pass an nearest neighbors estimator where `n_jobs` is already set instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Nityam\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\xgboost\\sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9946387132911899\n",
      "ROC AUC: 0.9542357258156992\n",
      "\n",
      "Classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     12424\n",
      "           1       0.56      0.37      0.45        73\n",
      "\n",
      "    accuracy                           0.99     12497\n",
      "   macro avg       0.78      0.68      0.72     12497\n",
      "weighted avg       0.99      0.99      0.99     12497\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      " [[12403    21]\n",
      " [   46    27]]\n",
      "\n",
      "Top features:\n",
      " risk_score_2             0.197984\n",
      "uid_nid_matches          0.102942\n",
      "risk_score_3             0.057174\n",
      "return_payments          0.049833\n",
      "merchant1_ten            0.049214\n",
      "spend                    0.045317\n",
      "no_trd_del               0.044527\n",
      "bureau_mismatch          0.036191\n",
      "bureau_score             0.032760\n",
      "no_addr_to_nid           0.024577\n",
      "basic_old_open_trd_ms    0.023666\n",
      "risk_score_5             0.022720\n",
      "limit                    0.021963\n",
      "dl_id                    0.020755\n",
      "merchant3_cat            0.019035\n",
      "merchant2_cat            0.018045\n",
      "basic_no_trd             0.017321\n",
      "decline_txn              0.016705\n",
      "merchant2_ten            0.016542\n",
      "addr_ds                  0.015927\n",
      "dtype: float32\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Apply SMOTE then train XGBoost on numeric features (uses existing `df`, `numeric_cols`, `target_col`)\n",
    "# Minimal preprocessing: median imputation for numeric cols\n",
    "\n",
    "# imports (safe to rerun)\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, roc_auc_score, confusion_matrix, accuracy_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from xgboost import XGBClassifier\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# prepare data\n",
    "X = df[numeric_cols].copy()\n",
    "y = df[target_col].copy()\n",
    "\n",
    "# impute any numeric NA with median (numeric_cols expected numeric)\n",
    "X = X.fillna(X.median())\n",
    "\n",
    "# train / test split (stratified)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "# SMOTE on training set\n",
    "sm = SMOTE(random_state=42, n_jobs=-1)\n",
    "X_res, y_res = sm.fit_resample(X_train, y_train)\n",
    "\n",
    "# fit XGBoost classifier\n",
    "model = XGBClassifier(\n",
    "    n_estimators=1000,\n",
    "    learning_rate=0.05,\n",
    "    use_label_encoder=False,\n",
    "    eval_metric='auc',\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "model.fit(\n",
    "    X_res, y_res,\n",
    "    early_stopping_rounds=50,\n",
    "    eval_set=[(X_test, y_test)],\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "# predictions & evaluation\n",
    "y_pred = model.predict(X_test)\n",
    "y_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"ROC AUC:\", roc_auc_score(y_test, y_proba))\n",
    "print(\"\\nClassification report:\\n\", classification_report(y_test, y_pred))\n",
    "print(\"\\nConfusion matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "\n",
    "# feature importance (top 20)\n",
    "fi = pd.Series(model.feature_importances_, index=X.columns).sort_values(ascending=False)\n",
    "print(\"\\nTop features:\\n\", fi.head(20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7cc0337c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: shap_plots/shap_summary_group_1.png\n",
      "Saved: shap_plots/shap_summary_group_2.png\n",
      "Saved: shap_plots/shap_summary_group_3.png\n",
      "Saved: shap_plots/shap_summary_group_4.png\n",
      "Saved: shap_plots/shap_summary_group_5.png\n",
      "Saved: shap_plots/shap_summary_group_6.png\n",
      "All grouped SHAP summary plots generated.\n"
     ]
    }
   ],
   "source": [
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "os.makedirs('shap_plots', exist_ok=True)\n",
    "\n",
    "# Use test set if available\n",
    "if 'X_test' in globals():\n",
    "    X = X_test.copy()\n",
    "else:\n",
    "    X = df[numeric_cols].copy().fillna(df[numeric_cols].median())\n",
    "\n",
    "# Compute SHAP values\n",
    "try:\n",
    "    explainer = shap.TreeExplainer(model)\n",
    "    shap_vals = explainer.shap_values(X)\n",
    "except:\n",
    "    explainer = shap.Explainer(model, X)\n",
    "    shap_vals = explainer(X).values\n",
    "\n",
    "# Handle multi-class\n",
    "if isinstance(shap_vals, list):\n",
    "    shap_arr = shap_vals[1] if len(shap_vals) > 1 else shap_vals[0]\n",
    "else:\n",
    "    shap_arr = shap_vals\n",
    "\n",
    "# ---------------------------\n",
    "# GROUP FEATURES (10 per plot)\n",
    "# ---------------------------\n",
    "features = list(X.columns)\n",
    "group_size = 10\n",
    "groups = [features[i:i+group_size] for i in range(0, len(features), group_size)]\n",
    "\n",
    "plot_no = 1\n",
    "\n",
    "for group in groups:\n",
    "\n",
    "    # Subset data & shap\n",
    "    X_sub = X[group]\n",
    "    shap_sub = shap_arr[:, [X.columns.get_loc(c) for c in group]]\n",
    "\n",
    "    # Create single summary plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    shap.summary_plot(shap_sub, X_sub, show=False)\n",
    "    plt.title(f\"SHAP Summary Plot (Features {plot_no})\")\n",
    "\n",
    "    # Save\n",
    "    fname = f\"shap_plots/shap_summary_group_{plot_no}.png\"\n",
    "    plt.savefig(fname, dpi=150, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    print(f\"Saved: {fname}\")\n",
    "    plot_no += 1\n",
    "\n",
    "print(\"All grouped SHAP summary plots generated.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34cdee22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer\n",
    "from reportlab.lib.styles import getSampleStyleSheet\n",
    "from reportlab.lib.pagesizes import A4\n",
    "\n",
    "text = \"\"\"‚ÄúHow do I find true fraud when I only have labels for defaults, and fraud is just a hidden minority inside that default population?‚Äù\n",
    "\n",
    "Let‚Äôs unpack everything very clearly, step by step, so there‚Äôs zero ambiguity.\n",
    "\n",
    "1. Problem Setup ‚Äî Why This Is Hard\n",
    "1.1 What labels you actually have\n",
    "\n",
    "The dataset gives you labels for defaulters (e.g., DEFAULT_IND = 1 or similar).\n",
    "\n",
    "In reality, only some of those defaulters are fraudsters.\n",
    "\n",
    "Many defaulters are just:\n",
    "\n",
    "genuinely over-leveraged,\n",
    "\n",
    "hit by financial hardship,\n",
    "\n",
    "or making poor but non-fraudulent decisions.\n",
    "\n",
    "So if you treat all defaulters as fraud, you are:\n",
    "\n",
    "Over-labeling ‚Üí you are calling many ‚Äúhonest but unlucky‚Äù customers fraud.\n",
    "\n",
    "Mixing behavioral patterns of fraudsters with normal defaulters.\n",
    "\n",
    "1.2 The true target: Fraud, not Default\n",
    "\n",
    "Business goal: detect fraud (intentional deception), not just high risk / non-payment.\n",
    "\n",
    "Technical reality: fraud label is missing ‚Üí no direct supervised signal for fraud.\n",
    "\n",
    "So:\n",
    "\n",
    "You have labels for default (proxy label).\n",
    "\n",
    "Fraud is a ‚Äúminority within a minority‚Äù:\n",
    "\n",
    "Overall population ‚Üí many good customers.\n",
    "\n",
    "A small subset ‚Üí defaulters.\n",
    "\n",
    "Inside that subset ‚Üí an even smaller, hidden subset = fraudsters.\n",
    "\n",
    "This becomes:\n",
    "\n",
    "A weakly supervised + anomaly detection problem where:\n",
    "\n",
    "‚ÄúDefault‚Äù provides a weak proxy for ‚Äúrisk/fraud potential‚Äù.\n",
    "\n",
    "‚ÄúFraud‚Äù must be discovered as anomalies inside that default/high-risk region.\n",
    "\n",
    "Supervised learning alone cannot solve this because:\n",
    "\n",
    "You don‚Äôt have clean fraud labels.\n",
    "\n",
    "Training a classifier on DEFAULT_IND gives you a default model, not a true fraud model.\n",
    "\n",
    "Fraud behavior is not the same as generic default behavior.\n",
    "\n",
    "Many defaulters behave in ‚Äúnormal but risky‚Äù ways.\n",
    "\n",
    "Fraudsters often show weird, inconsistent, or extreme patterns.\n",
    "\n",
    "Fraud is rarer than defaults.\n",
    "\n",
    "A standard default model will be optimized to catch all defaulters, not to differentiate the special few who are fraud.\n",
    "\n",
    "So you need something more nuanced.\n",
    "\n",
    "2. Overall Solution ‚Äî A Two-Stage Hybrid System\n",
    "\n",
    "To handle this, you designed a two-layer pipeline:\n",
    "\n",
    "Stage 1 (Supervised):\n",
    "Use default labels as a weak supervision layer to detect high-risk / suspicious behavior, not to directly detect fraud.\n",
    "\n",
    "Output: A filtered subset of customers who are ‚Äúhigh risk‚Äù, where fraud is more likely to be hiding.\n",
    "\n",
    "Think of this as narrowing down the haystack.\n",
    "\n",
    "Stage 2 (Unsupervised):\n",
    "On this risky subset, use anomaly detection to isolate true fraud-like outliers:\n",
    "\n",
    "Global anomalies with Isolation Forest\n",
    "\n",
    "Local/density-based anomalies with K-Means\n",
    "\n",
    "Combine the two into a dual anomaly score.\n",
    "\n",
    "So conceptually:\n",
    "\n",
    "Full population ‚Üí Supervised Risk Model ‚Üí High-risk subset ‚Üí Anomaly models ‚Üí Fraud candidates\n",
    "\n",
    "This pipeline respects reality:\n",
    "\n",
    "You use the labels you have (defaults) to narrow the field.\n",
    "\n",
    "You use unsupervised learning to search for fraud where it is most likely to exist: among the riskiest accounts.\n",
    "\n",
    "3. Stage 1 in Detail ‚Äî Supervised ‚ÄúWeak Labeling‚Äù Layer\n",
    "3.1 Input Features\n",
    "\n",
    "You worked with multiple feature families:\n",
    "\n",
    "Transactional features\n",
    "e.g., transaction amount patterns, velocity, frequency, time-of-day activity, cross-border transactions.\n",
    "\n",
    "Behavioral features\n",
    "e.g., payment regularity, utilization rate, sudden changes in usage, delinquency history.\n",
    "\n",
    "Financial / profile features\n",
    "e.g., income proxies, credit limits, product type, tenure, geography.\n",
    "\n",
    "These capture how a customer uses the product and how their risk evolves over time.\n",
    "\n",
    "3.2 Variable Selection with Information Value (IV)\n",
    "\n",
    "Before modeling, you did IV-based feature selection:\n",
    "\n",
    "What is IV (Information Value)?\n",
    "A measure widely used in credit risk to quantify how predictive a variable is for a binary outcome (here: default vs non-default).\n",
    "\n",
    "Why use IV?\n",
    "\n",
    "To rank variables by their predictive power.\n",
    "\n",
    "To drop uninformative or noisy variables.\n",
    "\n",
    "To stabilize the model and reduce overfitting.\n",
    "\n",
    "You:\n",
    "\n",
    "Calculated IV for each variable with respect to DEFAULT_IND.\n",
    "\n",
    "Kept variables above a certain threshold.\n",
    "\n",
    "Removed low-IV or redundant variables.\n",
    "\n",
    "This cleaned up the feature space and directly contributed to:\n",
    "\n",
    "‚úÖ 12% improvement in F1 score for the XGBoost model after IV-based filtering.\n",
    "\n",
    "3.3 Training XGBoost ‚Äî But With a Purpose\n",
    "\n",
    "You used XGBoost as the supervised model, trained on default labels, with this mindset:\n",
    "\n",
    "You are not predicting ‚Äúfraud = 1‚Äù.\n",
    "\n",
    "You are predicting:\n",
    "\n",
    "‚ÄúHigh-risk default behavior that is correlated with eventual loss‚Äù\n",
    "\n",
    "This model does two things:\n",
    "\n",
    "Learns nonlinear interactions and complex patterns that correlate with default.\n",
    "\n",
    "Produces a risk score for each customer:\n",
    "\n",
    "Higher score ‚Üí more default-like, riskier behavior.\n",
    "\n",
    "Lower score ‚Üí more normal, low-risk behavior.\n",
    "\n",
    "You then:\n",
    "\n",
    "Sorted customers by risk score.\n",
    "\n",
    "Focused Stage 2 anomaly detection on the high-risk tail (e.g., top X% by score or defaulters with high predicted probabilities).\n",
    "\n",
    "So Stage 1 is not a final fraud classifier.\n",
    "It is a filter to define the ‚Äúsearch region‚Äù for fraud.\n",
    "\n",
    "3.4 Model Explainability with SHAP\n",
    "\n",
    "You used SHAP (SHapley Additive exPlanations) to:\n",
    "\n",
    "Globally:\n",
    "\n",
    "Understand which features most drive default risk.\n",
    "\n",
    "Validate that the model aligns with business intuition (e.g., very high utilization, erratic repayments, etc., should increase risk).\n",
    "\n",
    "Locally (per customer):\n",
    "\n",
    "For a given account, see:\n",
    "\n",
    "Which factors push risk up.\n",
    "\n",
    "Which factors pull risk down.\n",
    "\n",
    "Benefits of SHAP here:\n",
    "\n",
    "Trust-building with risk/fraud stakeholders:\n",
    "\n",
    "They can see why someone is tagged as high-risk.\n",
    "\n",
    "Feature refinement:\n",
    "\n",
    "Using SHAP insight, you can engineer better features, drop misleading ones, and improve IV + model together.\n",
    "\n",
    "Operational transparency:\n",
    "\n",
    "When a case is escalated, analysts can see which behaviors drove the high-risk signal.\n",
    "\n",
    "This SHAP-driven iterative refinement contributed to the 12% F1 uplift and made the Stage 1 filter reliable enough to build Stage 2 on top of it.\n",
    "\n",
    "3.5 Role of Stage 1 (Conceptually)\n",
    "\n",
    "Stage 1 serves to:\n",
    "\n",
    "Reduce the search space:\n",
    "\n",
    "From 100% of customers ‚Üí only the riskiest portion.\n",
    "\n",
    "Increase fraud density:\n",
    "\n",
    "Fraudsters are more likely to be in the top-risk group than in the whole population.\n",
    "\n",
    "Make the data manageable for unsupervised analysis:\n",
    "\n",
    "Anomaly detection on the full population would be noisy, unstable, and less meaningful.\n",
    "\n",
    "So:\n",
    "\n",
    "Stage 1 = ‚ÄúDefine where to look‚Äù\n",
    "Stage 2 = ‚ÄúFigure out what is truly suspicious within that region‚Äù\n",
    "\n",
    "4. Stage 2 in Detail ‚Äî True Fraud Discovery via Anomalies\n",
    "\n",
    "Stage 2 operates on the filtered subset from Stage 1:\n",
    "\n",
    "Typically high-risk defaulters / top-risk scores where fraud is more likely.\n",
    "\n",
    "Here, you no longer rely on labels.\n",
    "Instead, you ask the question:\n",
    "\n",
    "‚ÄúWithin this already-risky group, who behaves so differently that they look like fraud?‚Äù\n",
    "\n",
    "You used two complementary anomaly detection techniques:\n",
    "\n",
    "4.1 Isolation Forest ‚Äî Global Outlier Detection\n",
    "\n",
    "What it does:\n",
    "\n",
    "Isolation Forest works by randomly partitioning the feature space:\n",
    "\n",
    "If a point is an outlier, it can be isolated with fewer splits.\n",
    "\n",
    "The average path length in the trees informs an anomaly score.\n",
    "\n",
    "What it captures in your context:\n",
    "\n",
    "Global outliers:\n",
    "\n",
    "People whose patterns (spend amount, frequency, geography, category mix, etc.) are extremely far from the majority of high-risk defaulters.\n",
    "\n",
    "Examples:\n",
    "\n",
    "Someone doing unusually high cross-border spends.\n",
    "\n",
    "Sudden extreme cash withdrawals inconsistent with their profile.\n",
    "\n",
    "Abnormal spike in high-risk merchant categories.\n",
    "\n",
    "Output:\n",
    "\n",
    "Each account gets a global anomaly score (higher score = more anomalous).\n",
    "\n",
    "4.2 K-Means ‚Äî Local / Density-based Anomalies\n",
    "\n",
    "What it does:\n",
    "\n",
    "K-Means clusters similar data points into K groups.\n",
    "\n",
    "Each cluster has a centroid (average behavior).\n",
    "\n",
    "The distance from a point to its cluster centroid can be used as a local anomaly score:\n",
    "\n",
    "Large distance = behaves differently from peers in that cluster.\n",
    "\n",
    "What it captures in your case:\n",
    "\n",
    "Local/density-based anomalies:\n",
    "\n",
    "Small behavioral groups that are unusual relative to the defaulter segment they belong to.\n",
    "\n",
    "Examples:\n",
    "\n",
    "Within a cluster of high-utilization, low-income customers, one subgroup shows:\n",
    "\n",
    "Unusual merchant types,\n",
    "\n",
    "Or a strange temporal transaction pattern,\n",
    "\n",
    "Or inconsistent repayment trends.\n",
    "\n",
    "These might not look extreme globally but are weird within their local neighborhood.\n",
    "\n",
    "Output:\n",
    "\n",
    "Each account gets a local anomaly score (distance-to-centroid-based).\n",
    "\n",
    "4.3 Why Both Models Are Needed\n",
    "\n",
    "Isolation Forest:\n",
    "\n",
    "Good for big, obvious outliers.\n",
    "\n",
    "Might miss subtle but suspicious micro-behaviors.\n",
    "\n",
    "K-Means:\n",
    "\n",
    "Good for finding oddballs within a cluster.\n",
    "\n",
    "Might struggle with extremely global outliers or poor cluster assignment alone.\n",
    "\n",
    "By combining them, you:\n",
    "\n",
    "Capture:\n",
    "\n",
    "Extremely strange patterns (via Isolation Forest).\n",
    "\n",
    "Subtle but locally odd patterns (via K-Means).\n",
    "\n",
    "Avoid relying on a single notion of ‚Äúanomaly‚Äù, which could be biased.\n",
    "\n",
    "4.4 Dual-Ensemble Anomaly Score\n",
    "\n",
    "You combined:\n",
    "\n",
    "Global anomaly score from Isolation Forest.\n",
    "\n",
    "Local anomaly score from K-Means (distance to centroid).\n",
    "\n",
    "Typical way conceptually (even if you didn‚Äôt code it exactly this way):\n",
    "\n",
    "Normalize both scores to a comparable scale (e.g., 0‚Äì1).\n",
    "\n",
    "Create a combined score like:\n",
    "\n",
    "FraudScore\n",
    "=\n",
    "ùõº\n",
    "‚ãÖ\n",
    "GlobalScore\n",
    "+\n",
    "(\n",
    "1\n",
    "‚àí\n",
    "ùõº\n",
    ")\n",
    "‚ãÖ\n",
    "LocalScore\n",
    "FraudScore=Œ±‚ãÖGlobalScore+(1‚àíŒ±)‚ãÖLocalScore\n",
    "\n",
    "where \n",
    "ùõº\n",
    "Œ± controls the importance of global vs local anomalies.\n",
    "\n",
    "Rank customers by this combined score.\n",
    "\n",
    "Top-ranked customers are fraud suspects:\n",
    "\n",
    "High risk from Stage 1.\n",
    "\n",
    "Anomalous both globally and/or locally in Stage 2.\n",
    "\n",
    "5. Final Outcomes and Metrics ‚Äî What Improved, Exactly?\n",
    "\n",
    "Your system delivered these key improvements:\n",
    "\n",
    "5.1 Detection Quality\n",
    "\n",
    "Recall ‚âà 75%\n",
    "\n",
    "You were able to capture ~75% of the (hidden) fraud cases.\n",
    "\n",
    "Compared to a baseline, this means you are finding more fraud that would otherwise be missed.\n",
    "\n",
    "False Positive Rate ‚Üì ~30%\n",
    "\n",
    "Even though you increased recall, you reduced FPR by ~30%.\n",
    "\n",
    "This means:\n",
    "\n",
    "Fewer genuine customers are being wrongly flagged.\n",
    "\n",
    "Fraud analysts spend less time on non-fraud cases.\n",
    "\n",
    "This precision‚Äìrecall trade-off is better because:\n",
    "\n",
    "Stage 1 focuses on the high-risk segment.\n",
    "\n",
    "Stage 2 refines that segment using robust anomaly scoring.\n",
    "\n",
    "5.2 Operational Efficiency\n",
    "\n",
    "You didn‚Äôt stop at just model metrics. You built tools around the model:\n",
    "\n",
    "Streamlit App\n",
    "\n",
    "For interactive:\n",
    "\n",
    "Exploration of the model outputs.\n",
    "\n",
    "SHAP-based explanations of why someone is deemed high risk.\n",
    "\n",
    "Viewing anomaly scores for individual accounts.\n",
    "\n",
    "Useful for:\n",
    "\n",
    "Data scientists / risk teams to experiment with thresholds.\n",
    "\n",
    "Sanity check on model behavior.\n",
    "\n",
    "Power BI Dashboards\n",
    "\n",
    "For fraud analysts & management:\n",
    "\n",
    "View distributions of fraud scores.\n",
    "\n",
    "Monitor volumes of flagged cases.\n",
    "\n",
    "Track trends over time (e.g., fraud score by region, product, merchant type).\n",
    "\n",
    "Provides operational transparency and continuous monitoring.\n",
    "\n",
    "Business impact:\n",
    "\n",
    "‚úÖ Fraud investigation efficiency increased by ~25%\n",
    "\n",
    "Meaning:\n",
    "\n",
    "Analysts need to spend less time sifting through irrelevant alerts.\n",
    "\n",
    "They are guided towards cases with higher anomaly + risk scores.\n",
    "\n",
    "More fraud discovered for the same or even lower effort.\n",
    "\n",
    "6. Why This Methodology Is Justified (and Smart)\n",
    "\n",
    "Let‚Äôs tie the logic together:\n",
    "\n",
    "Supervised alone doesn‚Äôt work:\n",
    "\n",
    "Default labels ‚â† fraud labels.\n",
    "\n",
    "A default model will learn:\n",
    "\n",
    "‚ÄúWho tends to not pay back?‚Äù\n",
    "not\n",
    "\n",
    "‚ÄúWho is actively committing fraud?‚Äù\n",
    "\n",
    "It confuses financial struggle with deception.\n",
    "\n",
    "Unsupervised alone doesn‚Äôt work well either:\n",
    "\n",
    "Running anomaly detection on all customers:\n",
    "\n",
    "Lots of noise from naturally diverse behaviors.\n",
    "\n",
    "Very high false positives.\n",
    "\n",
    "Little business focus.\n",
    "\n",
    "Your two-stage hybrid method fixes this:\n",
    "\n",
    "Stage 1 (XGBoost + IV + SHAP) uses default as a weak but valuable signal:\n",
    "\n",
    "It narrows the search space to high-risk behavior.\n",
    "\n",
    "It is interpretable and aligns with credit risk best practices.\n",
    "\n",
    "Stage 2 (Isolation Forest + K-Means) focuses only on that high-risk region:\n",
    "\n",
    "Finds outliers among outliers.\n",
    "\n",
    "Targets the ‚Äúminority within the minority‚Äù structure of fraud within defaulters.\n",
    "\n",
    "Dual anomaly scoring:\n",
    "\n",
    "Merges global and local perspectives on abnormality.\n",
    "\n",
    "Explainability built-in:\n",
    "\n",
    "IV + SHAP = transparent feature selection and model behavior.\n",
    "\n",
    "Analysts and risk managers can understand:\n",
    "\n",
    "Why someone is in the high-risk filter (Stage 1).\n",
    "\n",
    "Why they are flagged as anomalous (Stage 2 scores & patterns).\n",
    "\n",
    "Operational readiness:\n",
    "\n",
    "Streamlit ‚Üí experimentation & what-if analysis.\n",
    "\n",
    "Power BI ‚Üí live monitoring & business reporting.\n",
    "\n",
    "This is not just a ‚ÄúKaggle model‚Äù; it‚Äôs a full fraud detection workflow.\n",
    "\"\"\"\n",
    "\n",
    "doc = SimpleDocTemplate(\"/mnt/data/fraud_project_explanation.pdf\", pagesize=A4)\n",
    "styles = getSampleStyleSheet()\n",
    "story = []\n",
    "\n",
    "for line in text.split(\"\\n\"):\n",
    "    story.append(Paragraph(line, styles['Normal']))\n",
    "    story.append(Spacer(1, 12))\n",
    "\n",
    "doc.build(story)\n",
    "\n",
    "\"/mnt/data/fraud_project_explanation.pdf\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
